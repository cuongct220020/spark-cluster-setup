version: '3.8'

networks:
  spark-network:
    driver: bridge

volumes:
  spark-events:

services:
  # ============================================
  # Spark Master Node (Single - No HA)
  # ============================================
  spark-master:
    image: ${SPARK_IMAGE}
    container_name: spark-master
    hostname: spark-master
    networks:
      - spark-network
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
      SPARK_PUBLIC_DNS: spark-master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_MASTER_OPTS: >-
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - spark-events:/opt/spark/spark-events
#      - ./spark/jars:/opt/spark/jars
#      - ./apps:/opt/spark/apps
    command: >
      bash -c "
        echo 'Creating event log directory...';
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # ============================================
  # Spark Worker Nodes (3 workers)
  # ============================================
  spark-worker-1:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-1
    hostname: spark-worker-1
    networks:
      - spark-network
    ports:
      - "8081:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-1
      SPARK_WORKER_HOST: spark-worker-1
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_WORKER_OPTS: >-
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/worker-1-data:/opt/spark/work-dir
#      - ./spark/jars:/opt/spark/jars
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Spark Master...';
        echo 'Starting Spark Worker 1...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  spark-worker-2:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-2
    hostname: spark-worker-2
    networks:
      - spark-network
    ports:
      - "8082:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-2
      SPARK_WORKER_HOST: spark-worker-2
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_WORKER_OPTS: >-
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/worker-2-data:/opt/spark/work-dir
#      - ./spark/jars:/opt/spark/jars
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Spark Master...';
        echo 'Starting Spark Worker 2...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  spark-worker-3:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-3
    hostname: spark-worker-3
    networks:
      - spark-network
    ports:
      - "8083:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-3
      SPARK_WORKER_HOST: spark-worker-3
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY:-1g}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_WORKER_OPTS: >-
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/worker-3-data:/opt/spark/work-dir
#      - ./spark/jars:/opt/spark/jars
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Spark Master...';
        echo 'Starting Spark Worker 3...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  # ============================================
  # Spark History Server
  # ============================================
  spark-history:
    image: ${SPARK_IMAGE}
    container_name: spark-history
    hostname: spark-history
    networks:
      - spark-network
    ports:
      - "18080:18080"
    environment:
      SPARK_NO_DAEMONIZE: true
      SPARK_PUBLIC_DNS: spark-history
      SPARK_HISTORY_OPTS: >-
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
        -Dspark.history.retainedApplications=${SPARK_HISTORY_RETAINED_APP}
        -Dspark.history.ui.port=${SPARK_HISTORY_UI_PORT}
        -Dspark.history.fs.update.interval=10s
        -Dspark.history.fs.cleaner.enabled=true
        -Dspark.history.fs.cleaner.interval=1d
        -Dspark.history.fs.cleaner.maxAge=7d
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - spark-events:/opt/spark/spark-events:ro
    depends_on:
      spark-master:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for event logs...';
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark History Server...';
        /opt/spark/sbin/start-history-server.sh;
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 45s