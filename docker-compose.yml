version: '3.8'

networks:
  spark-network:
    driver: bridge

volumes:
  zk1-data:
  zk1-log:
  zk2-data:
  zk2-log:
  zk3-data:
  zk3-log:
  spark-events:

services:
  # ============================================
  # ZooKeeper Cluster (3 nodes for HA)
  # ============================================
  zookeeper-1:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-1
    hostname: zookeeper-1
    networks:
      - spark-network
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
    volumes:
      - ./zk1/zk1-data:/data
      - ./zk1/zk1-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  zookeeper-2:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-2
    hostname: zookeeper-2
    networks:
      - spark-network
    ports:
      - "2182:2181"
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
    volumes:
      - ./zk2/zk2-data:/data
      - ./zk2/zk2-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  zookeeper-3:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-3
    hostname: zookeeper-3
    networks:
      - spark-network
    ports:
      - "2183:2181"
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
    volumes:
      - ./zk3/zk3-data:/data
      - ./zk3/zk3-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # Spark Master Nodes (HA)
  # ============================================
  spark-master-1:
    image: ${SPARK_IMAGE}
    container_name: spark-master-1
    hostname: spark-master-1
    networks:
      - spark-network
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master-1
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_DAEMON_JAVA_OPTS: |
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE}
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL}
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR}
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
    volumes:
      - ./spark-events:/opt/spark/spark-events
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper to be ready...';
        sleep 10;
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master 1...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5

  spark-master-2:
    image: ${SPARK_IMAGE}
    container_name: spark-master-2
    hostname: spark-master-2
    networks:
      - spark-network
    ports:
      - "7078:7077"
      - "8081:8080"
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master-2
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_DAEMON_JAVA_OPTS: |
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE}
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL}
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR}
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
    volumes:
      - ./spark-events:/opt/spark/spark-events
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper to be ready...';
        sleep 10;
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master 2...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5

  spark-master-3:
    image: ${SPARK_IMAGE}
    container_name: spark-master-3
    hostname: spark-master-3
    networks:
      - spark-network
    ports:
      - "7079:7077"
      - "8082:8080"
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master-3
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_DAEMON_JAVA_OPTS: |
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE}
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL}
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR}
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
    volumes:
      - ./spark-events:/opt/spark/spark-events
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper to be ready...';
        sleep 10;
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master 3...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # Spark Worker Nodes
  # ============================================
  spark-worker-1:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-1
    hostname: spark-worker-1
    networks:
      - spark-network
    ports:
      - "8083:8081"
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_HOST: spark-worker-1
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark Masters...';
        sleep 15;
        echo 'Starting Spark Worker 1...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*"
    restart: unless-stopped

  spark-worker-2:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-2
    hostname: spark-worker-2
    networks:
      - spark-network
    ports:
      - "8084:8081"
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_HOST: spark-worker-2
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark Masters...';
        sleep 15;
        echo 'Starting Spark Worker 2...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*"
    restart: unless-stopped

  spark-worker-3:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-3
    hostname: spark-worker-3
    networks:
      - spark-network
    ports:
      - "8085:8081"
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_HOST: spark-worker-3
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark Masters...';
        sleep 15;
        echo 'Starting Spark Worker 3...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*"
    restart: unless-stopped

  # ============================================
  # Spark History Server
  # ============================================
  spark-history:
    image: ${SPARK_IMAGE}
    container_name: spark-history
    hostname: spark-history
    networks:
      - spark-network
    ports:
      - "18080:18080"
    environment:
      SPARK_NO_DAEMONIZE: "true"
      SPARK_HISTORY_OPTS: |
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
        -Dspark.history.retainedApplications=${SPARK_HISTORY_RETAINED_APP}
        -Dspark.history.ui.port=${SPARK_HISTORY_UI_PORT}
    volumes:
      - ./spark-events:/opt/spark/spark-events:ro
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark cluster to generate event logs...';
        sleep 20;
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark History Server...';
        /opt/spark/sbin/start-history-server.sh;
        tail -f /opt/spark/logs/*"
    restart: unless-stopped
