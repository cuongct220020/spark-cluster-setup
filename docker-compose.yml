version: '3.8'

networks:
  spark-network:
    driver: bridge

volumes:
  zk1-data:
  zk1-log:
  zk2-data:
  zk2-log:
  zk3-data:
  zk3-log:
  spark-events:

services:
  # ============================================
  # ZooKeeper Cluster (3 nodes for HA)
  # ============================================
  zookeeper-1:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-1
    hostname: zookeeper-1
    networks:
      - spark-network
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
      ZOO_MAX_SESSION_TIMEOUT: ${ZOO_MAX_SESSION_TIMEOUT}
      ZOO_MIN_SESSION_TIMEOUT: ${ZOO_MIN_SESSION_TIMEOUT}
      ZOO_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./zk1/zk1-data:/data
      - ./zk1/zk1-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  zookeeper-2:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-2
    hostname: zookeeper-2
    networks:
      - spark-network
    ports:
      - "2182:2181"
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
      ZOO_MAX_SESSION_TIMEOUT: ${ZOO_MAX_SESSION_TIMEOUT}
      ZOO_MIN_SESSION_TIMEOUT: ${ZOO_MIN_SESSION_TIMEOUT}
      ZOO_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./zk2/zk2-data:/data
      - ./zk2/zk2-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  zookeeper-3:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-3
    hostname: zookeeper-3
    networks:
      - spark-network
    ports:
      - "2183:2181"
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
      ZOO_MAX_SESSION_TIMEOUT: ${ZOO_MAX_SESSION_TIMEOUT}
      ZOO_MIN_SESSION_TIMEOUT: ${ZOO_MIN_SESSION_TIMEOUT}
      ZOO_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./zk3/zk3-data:/data
      - ./zk3/zk3-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # Spark Master Nodes (HA)
  # ============================================
  spark-master-1:
    image: ${SPARK_IMAGE}
    container_name: spark-master-1
    hostname: spark-master-1
    networks:
      - spark-network
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: "true"
      SPARK_PUBLIC_DNS: spark-master-1
      SPARK_MASTER_HOST: spark-master-1
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY}
      SPARK_DAEMON_JAVA_OPTS: "
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE} \
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL} \
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR} \
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED} \
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR} \
        -Dspark.eventLog.rolling.enabled=${SPARK_EVENTLOG_ROLLING_ENABLED} \
        -Dspark.eventLog.rolling.maxFileSize=${SPARK_EVENTLOG_ROLLING_MAX_FILE_SIZE} \
        -Dspark.eventLog.rolling.maxRetainedFiles=${SPARK_EVENTLOG_ROLLING_MAX_RETAINED_FILES} \
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR} \
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED} \
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET} \
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED} \
        -XX:+UseG1GC
        "
      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
      SPARK_DRIVER_EXTRA_CLASSPATH: /opt/spark/jars/*
      SPARK_EXECUTOR_EXTRA_CLASSPATH: /opt/spark/jars/*
      TZ: Asia/Ho_Chi_Minh
#      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
#      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
#      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
    volumes:
      - ./spark-events:/opt/spark/spark-events
      - ./spark/jars:/opt/spark/jars
      - ./apps:/opt/spark/apps
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper...';
        sleep 10;
        mkdir -p /opt/spark/spark-events;
        /opt/spark/sbin/start-master.sh
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 5

  spark-master-2:
    image: ${SPARK_IMAGE}
    container_name: spark-master-2
    hostname: spark-master-2
    networks:
      - spark-network
    ports:
      - "7078:7077"
      - "8081:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: "true"
      SPARK_PUBLIC_DNS: spark-master-2
      SPARK_MASTER_HOST: spark-master-2
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY}
      SPARK_DAEMON_JAVA_OPTS: "
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE} \
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL} \
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR} \
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED} \
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR} \
        -Dspark.eventLog.rolling.enabled=${SPARK_EVENTLOG_ROLLING_ENABLED} \
        -Dspark.eventLog.rolling.maxFileSize=${SPARK_EVENTLOG_ROLLING_MAX_FILE_SIZE} \
        -Dspark.eventLog.rolling.maxRetainedFiles=${SPARK_EVENTLOG_ROLLING_MAX_RETAINED_FILES} \
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR} \
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED} \
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET} \
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED} \
        -XX:+UseG1GC
        "
      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
      SPARK_DRIVER_EXTRA_CLASSPATH: /opt/spark/jars/*
      SPARK_EXECUTOR_EXTRA_CLASSPATH: /opt/spark/jars/*
      TZ: Asia/Ho_Chi_Minh
#      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
#      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
#      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
    volumes:
      - ./spark-events:/opt/spark/spark-events
      - ./spark/jars:/opt/spark/jars
      - ./apps:/opt/spark/apps
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper...';
        sleep 10;
        mkdir -p /opt/spark/spark-events;
        /opt/spark/sbin/start-master.sh
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 5

  spark-master-3:
    image: ${SPARK_IMAGE}
    container_name: spark-master-3
    hostname: spark-master-3
    networks:
      - spark-network
    ports:
      - "7079:7077"
      - "8082:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: "true"
      SPARK_PUBLIC_DNS: spark-master-3
      SPARK_MASTER_HOST: spark-master-3
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY}
      SPARK_DAEMON_JAVA_OPTS: "
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE} \
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL} \
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR} \
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED} \
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR} \
        -Dspark.eventLog.rolling.enabled=${SPARK_EVENTLOG_ROLLING_ENABLED} \
        -Dspark.eventLog.rolling.maxFileSize=${SPARK_EVENTLOG_ROLLING_MAX_FILE_SIZE} \
        -Dspark.eventLog.rolling.maxRetainedFiles=${SPARK_EVENTLOG_ROLLING_MAX_RETAINED_FILES} \
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR} \
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED} \
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET} \
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED} \
        -XX:+UseG1GC
        "
      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
      SPARK_DRIVER_EXTRA_CLASSPATH: /opt/spark/jars/*
      SPARK_EXECUTOR_EXTRA_CLASSPATH: /opt/spark/jars/*
      TZ: Asia/Ho_Chi_Minh
#      KAFKA_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}
#      CLICKHOUSE_HOST: ${CLICKHOUSE_HOST}
#      CLICKHOUSE_PORT: ${CLICKHOUSE_PORT}
    volumes:
      - ./spark-events:/opt/spark/spark-events
      - ./spark/jars:/opt/spark/jars
      - ./apps:/opt/spark/apps
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper...';
        sleep 10;
        mkdir -p /opt/spark/spark-events;
        /opt/spark/sbin/start-master.sh
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ============================================
  # Spark Worker Nodes
  # ============================================
  spark-worker-1:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-1
    hostname: spark-worker-1
    networks:
      - spark-network
    ports:
      - "8083:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-1
      SPARK_WORKER_HOST: spark-worker-1
      SPARK_NO_DAEMONIZE: ${SPARK_NO_DAEMONIZE}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
      SPARK_DAEMON_JAVA_OPTS: "
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED} \
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET} \
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      "
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/jars:/opt/spark/jars
      - ./spark/worker-data:/opt/spark/work-dir
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark Masters...';
        sleep 15;
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL}
      "
    restart: unless-stopped

  spark-worker-2:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-2
    hostname: spark-worker-2
    networks:
      - spark-network
    ports:
      - "8084:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-2
      SPARK_WORKER_HOST: spark-worker-2
      SPARK_NO_DAEMONIZE: ${SPARK_NO_DAEMONIZE}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
      SPARK_DAEMON_JAVA_OPTS: "
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED} \
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET} \
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      "
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/jars:/opt/spark/jars
      - ./spark/worker-data:/opt/spark/work-dir
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark Masters...';
        sleep 15;
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL}
      "
    restart: unless-stopped

  spark-worker-3:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-3
    hostname: spark-worker-3
    networks:
      - spark-network
    ports:
      - "8085:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-3
      SPARK_WORKER_HOST: spark-worker-3
      SPARK_NO_DAEMONIZE: ${SPARK_NO_DAEMONIZE}
      SPARK_DAEMON_MEMORY: ${SPARK_DAEMON_MEMORY}
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
      SPARK_DAEMON_JAVA_OPTS: "
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED} \
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET} \
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      "
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/jars:/opt/spark/jars
      - ./spark/worker-data:/opt/spark/work-dir
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark Masters...';
        sleep 15;
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL}
      "
    restart: unless-stopped

  # ============================================
  # Spark History Server
  # ============================================
  spark-history:
    image: ${SPARK_IMAGE}
    container_name: spark-history
    hostname: spark-history
    networks:
      - spark-network
    ports:
      - "18080:18080"
    environment:
      SPARK_NO_DAEMONIZE: ${SPARK_NO_DAEMONIZE}
      SPARK_PUBLIC_DNS: spark-history
      SPARK_HISTORY_OPTS: >
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
        -Dspark.history.retainedApplications=${SPARK_HISTORY_RETAINED_APP}
        -Dspark.history.ui.port=${SPARK_HISTORY_UI_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark-events:/opt/spark/spark-events:ro
    depends_on:
      - spark-master-1
      - spark-master-2
      - spark-master-3
    command: >
      bash -c "
        echo 'Waiting for Spark cluster to generate event logs...';
        sleep 20;
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark History Server...';
        /opt/spark/sbin/start-history-server.sh
      "
    restart: unless-stopped