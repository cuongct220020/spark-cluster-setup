version: '3.8'

networks:
  spark-network:
    driver: bridge

volumes:
  zk1-data:
  zk1-log:
  zk2-data:
  zk2-log:
  zk3-data:
  zk3-log:
  spark-events:

services:
  # ============================================
  # ZooKeeper Cluster (3 nodes for HA)
  # ============================================
  zookeeper-1:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-1
    hostname: zookeeper-1
    networks:
      - spark-network
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
      ZOO_MAX_SESSION_TIMEOUT: ${ZOO_MAX_SESSION_TIMEOUT}
      ZOO_MIN_SESSION_TIMEOUT: ${ZOO_MIN_SESSION_TIMEOUT}
      ZOO_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - zk1-data:/data
      - zk1-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok && echo stat | nc localhost 2181 | grep -q Mode"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  zookeeper-2:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-2
    hostname: zookeeper-2
    networks:
      - spark-network
    ports:
      - "2182:2181"
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
      ZOO_MAX_SESSION_TIMEOUT: ${ZOO_MAX_SESSION_TIMEOUT}
      ZOO_MIN_SESSION_TIMEOUT: ${ZOO_MIN_SESSION_TIMEOUT}
      ZOO_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - zk2-data:/data
      - zk2-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok && echo stat | nc localhost 2181 | grep -q Mode"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  zookeeper-3:
    image: ${ZOO_IMAGE}
    container_name: zookeeper-3
    hostname: zookeeper-3
    networks:
      - spark-network
    ports:
      - "2183:2181"
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: ${ZOO_SERVERS}
      ZOO_4LW_COMMANDS_WHITELIST: ${ZOO_4LW_COMMANDS_WHITELIST}
      ZOO_TICK_TIME: ${ZOO_TICK_TIME}
      ZOO_INIT_LIMIT: ${ZOO_INIT_LIMIT}
      ZOO_SYNC_LIMIT: ${ZOO_SYNC_LIMIT}
      ZOO_MAX_CLIENT_CNXNS: ${ZOO_MAX_CLIENT_CNXNS}
      ZOO_AUTOPURGE_SNAPRETAINCOUNT: ${ZOO_AUTOPURGE_SNAPRETAINCOUNT}
      ZOO_AUTOPURGE_PURGEINTERVAL: ${ZOO_AUTOPURGE_PURGEINTERVAL}
      ZOO_MAX_SESSION_TIMEOUT: ${ZOO_MAX_SESSION_TIMEOUT}
      ZOO_MIN_SESSION_TIMEOUT: ${ZOO_MIN_SESSION_TIMEOUT}
      ZOO_CLIENT_PORT: ${ZOO_CLIENT_PORT}
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - zk3-data:/data
      - zk3-log:/datalog
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok && echo stat | nc localhost 2181 | grep -q Mode"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  # ============================================
  # Spark Master Nodes (HA with ZooKeeper)
  # ============================================
  spark-master-1:
    image: ${SPARK_IMAGE}
    container_name: spark-master-1
    hostname: spark-master-1
    networks:
      - spark-network
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: 1g
      SPARK_PUBLIC_DNS: spark-master-1
      SPARK_MASTER_HOST: spark-master-1
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_MASTER_OPTS: >-
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE}
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL}
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR}
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
#      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
#      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - spark-events:/opt/spark/spark-events
#      - ./spark/jars:/opt/spark/jars
#      - ./apps:/opt/spark/apps
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper cluster to be ready...';
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master 1 with HA...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s

  spark-master-2:
    image: ${SPARK_IMAGE}
    container_name: spark-master-2
    hostname: spark-master-2
    networks:
      - spark-network
    ports:
      - "7078:7077"
      - "8081:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: 1G
      SPARK_PUBLIC_DNS: spark-master-2
      SPARK_MASTER_HOST: spark-master-2
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_MASTER_OPTS: >-
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE}
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL}
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR}
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
#      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
#      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - spark-events:/opt/spark/spark-events
#      - ./spark/jars:/opt/spark/jars
#      - ./apps:/opt/spark/apps
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper cluster to be ready...';
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master 2 with HA...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s

  spark-master-3:
    image: ${SPARK_IMAGE}
    container_name: spark-master-3
    hostname: spark-master-3
    networks:
      - spark-network
    ports:
      - "7079:7077"
      - "8082:8080"
    environment:
      SPARK_MODE: master
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: 1G
      SPARK_PUBLIC_DNS: spark-master-3
      SPARK_MASTER_HOST: spark-master-3
      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT}
      SPARK_MASTER_OPTS: >-
        -Dspark.deploy.recoveryMode=${SPARK_RECOVERY_MODE}
        -Dspark.deploy.zookeeper.url=${SPARK_ZK_URL}
        -Dspark.deploy.zookeeper.dir=${SPARK_ZK_DIR}
        -Dspark.eventLog.enabled=${SPARK_EVENTLOG_ENABLED}
        -Dspark.eventLog.dir=${SPARK_EVENTLOG_DIR}
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_MEMORY: ${SPARK_DRIVER_MEMORY}
#      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
#      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - spark-events:/opt/spark/spark-events
#      - ./spark/jars:/opt/spark/jars
#      - ./apps:/opt/spark/apps
    depends_on:
      zookeeper-1:
        condition: service_healthy
      zookeeper-2:
        condition: service_healthy
      zookeeper-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for ZooKeeper cluster to be ready...';
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark Master 3 with HA...';
        /opt/spark/sbin/start-master.sh;
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 60s

  # ============================================
  # Spark Worker Nodes
  # ============================================
  spark-worker-1:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-1
    hostname: spark-worker-1
    networks:
      - spark-network
    ports:
      - "8083:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-1
      SPARK_WORKER_HOST: spark-worker-1
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: 1G
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_WORKER_OPTS: >-
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
#      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/worker-1-data:/opt/spark/work-dir
#      - ./spark/jars:/opt/spark/jars
    depends_on:
      spark-master-1:
        condition: service_healthy
      spark-master-2:
        condition: service_healthy
      spark-master-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Spark Masters HA to be ready...';
        echo 'Starting Spark Worker 1...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  spark-worker-2:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-2
    hostname: spark-worker-2
    networks:
      - spark-network
    ports:
      - "8084:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-2
      SPARK_WORKER_HOST: spark-worker-2
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: 1G
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_WORKER_OPTS: >-
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
#      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/worker-2-data:/opt/spark/work-dir
#      - ./spark/jars:/opt/spark/jars
    depends_on:
      spark-master-1:
        condition: service_healthy
      spark-master-2:
        condition: service_healthy
      spark-master-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Spark Masters HA to be ready...';
        echo 'Starting Spark Worker 2...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  spark-worker-3:
    image: ${SPARK_IMAGE}
    container_name: spark-worker-3
    hostname: spark-worker-3
    networks:
      - spark-network
    ports:
      - "8085:8081"
    environment:
      SPARK_MODE: worker
      SPARK_PUBLIC_DNS: spark-worker-3
      SPARK_WORKER_HOST: spark-worker-3
      SPARK_NO_DAEMONIZE: true
      SPARK_DAEMON_MEMORY: 1G
      SPARK_MASTER_URL: ${SPARK_MASTER_URL}
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT}
      SPARK_LOCAL_DIRS: ${SPARK_LOCAL_DIRS}
      SPARK_WORKER_OPTS: >-
        -Dspark.authenticate=${SPARK_RPC_AUTHENTICATION_ENABLED}
        -Dspark.authenticate.secret=${SPARK_RPC_AUTHENTICATION_SECRET}
        -Dspark.network.crypto.enabled=${SPARK_RPC_ENCRYPTION_ENABLED}
      SPARK_DAEMON_JAVA_OPTS: >-
        -XX:+UseG1GC
        -XX:+UnlockExperimentalVMOptions
        -XX:MaxGCPauseMillis=200
#      SPARK_DRIVER_EXTRA_CLASSPATH: ${SPARK_DRIVER_EXTRA_CLASSPATH}
#      SPARK_EXECUTOR_EXTRA_CLASSPATH: ${SPARK_EXECUTOR_EXTRA_CLASSPATH}
#      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./spark/worker-3-data:/opt/spark/work-dir
#      - ./spark/jars:/opt/spark/jars
    depends_on:
      spark-master-1:
        condition: service_healthy
      spark-master-2:
        condition: service_healthy
      spark-master-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for Spark Masters HA to be ready...';
        echo 'Starting Spark Worker 3...';
        /opt/spark/sbin/start-worker.sh ${SPARK_MASTER_URL};
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 40s

  # ============================================
  # Spark History Server
  # ============================================
  spark-history:
    image: ${SPARK_IMAGE}
    container_name: spark-history
    hostname: spark-history
    networks:
      - spark-network
    ports:
      - "18080:18080"
    environment:
      SPARK_NO_DAEMONIZE: true
      SPARK_PUBLIC_DNS: spark-history
      SPARK_HISTORY_OPTS: >-
        -Dspark.history.fs.logDirectory=${SPARK_HISTORY_LOG_DIR}
        -Dspark.history.retainedApplications=${SPARK_HISTORY_RETAINED_APP}
        -Dspark.history.ui.port=${SPARK_HISTORY_UI_PORT}
        -Dspark.history.fs.update.interval=10s
        -Dspark.history.fs.cleaner.enabled=true
        -Dspark.history.fs.cleaner.interval=1d
        -Dspark.history.fs.cleaner.maxAge=7d
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - spark-events:/opt/spark/spark-events:ro
    depends_on:
      spark-master-1:
        condition: service_healthy
      spark-master-2:
        condition: service_healthy
      spark-master-3:
        condition: service_healthy
    command: >
      bash -c "
        echo 'Waiting for event logs...';
        mkdir -p /opt/spark/spark-events;
        echo 'Starting Spark History Server...';
        /opt/spark/sbin/start-history-server.sh;
        tail -f /opt/spark/logs/*
      "
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:18080"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 45s